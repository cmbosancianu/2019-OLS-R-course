\documentclass[12pt,english,pdf,dvipsnames,aspectratio=169]{beamer}
\usepackage{fontspec}
\setsansfont[Mapping=tex-text]{Fira Sans}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\usepackage[normalem]{ulem}
\usepackage[T1]{fontenc}
\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{setspace}
\makeatletter
\usetheme{metropolis}
\usepackage{mathpazo}
\usepackage{xcolor}
\definecolor{title}{RGB}{255,98,0}
\usepackage{tikz, tikz-cd}
\usetikzlibrary{shapes,backgrounds,trees,decorations.pathreplacing}
\usepackage[labelformat=empty]{caption}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepgfplotslibrary{fillbetween}
\usepackage{pgfplotstable}
\usepackage[sectionbib]{apacite}
\renewcommand{\bibliographytypesize}{\footnotesize}
\usepackage{amsmath}
\usepackage{polyglossia}
\setdefaultlanguage[variant=american]{english}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{hyperref}
\hypersetup{pdfauthor={Constantin Manuel Bosancianu},
pdftitle={Linear regression with R/Stata},
pdfsubject={Day 3: Categorical predictors and Regression inference},
pdfkeywords={Bamberg, ECPR, 2019, day 3, WSMT}}
% Defines a checkmark
\def\checkmark{\tikz\fill[scale=0.4, color=title](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\setbeamertemplate{itemize items}{\checkmark}
% Small underbrace
\def\smallunderbrace#1{\mathop{\vtop{\m@th\ialign{##\crcr
   $\hfil\displaystyle{#1}\hfil$\crcr
   \noalign{\kern3\p@\nointerlineskip}%
   \tiny\upbracefill\crcr\noalign{\kern3\p@}}}}\limits}
\usepackage{appendixnumberbeamer}
\title{\textsc{Linear Regression with R/Stata}}
\subtitle{Day 3: Categorical Predictors \& Inference}
\author{Constantin Manuel Bosancianu}
\institute{Wissenschaftszentrum Berlin \\ \textit{Institutions and Political Inequality} \\\href{mailto:bosancianu@icloud.com}{bosancianu@icloud.com}}
\date{February 27, 2019}
\begin{document}
\maketitle
% PREAMBLE %

<<options, eval=TRUE, echo=FALSE, message=FALSE>>=
library(pacman)
p_load(tidyverse, knitr, foreign, magrittr, texreg,
       ggthemes)

opts_chunk$set(message = FALSE,
               warning = FALSE,
               comment = NA,
               echo = FALSE,
               eval = TRUE)
@

\section{Preamble}

\begin{frame}{Recap from yesterday}

\begin{itemize}
\item Model fit: residual standard error \& $R^2$ (coefficient of determination);
\item (adjusted) $R^2$ is most used measure of fit;
\item Interpretation of coefficients in the simple and multiple case is identical;
\item Multiple regression coefficients are \textit{partial} coefficients---effect of $X$ while keeping all other predictors constant.
\end{itemize}

\end{frame}



\begin{frame}{NLSY example}

<<ch-1, results='asis'>>=
df_child <- read.dta(file = "../02-data/kidiq.dta",
                     convert.factors = FALSE)

# DATA FROM THE NATIONAL LONGITUDINAL SURVEY OF YOUTH
# "kid_score": score at age 3 on an IQ test
# "mom_hs": mother is a high school graduate
# "mom_iq": mother's IQ score
# "mom_work": whether mother worked when having baby (1=did not work for
#   3 years after birth; 2=worked after the first year of baby's life;
#   3=worked part-time in the first year of baby's life; 4=worked full-time
#   in the first year of baby's life);
# "mom_age": age of the mother at the time of baby's birth

# I will just do a bit of re-scaling, so that the interpretation of the
#   intercept is meaningful
df_child %<>%
    mutate(mom_iq = mom_iq - mean(mom_iq, na.rm = TRUE),
           mom_work = 4 - mom_work,
           mom_age = mom_age - 18)

model1 <- lm(kid_score ~ mom_iq + mom_hs + mom_age,
             data = df_child,
             na.action = na.omit)

texreg(list(model1),
       digits = 3,
       single.row = FALSE,
       custom.coef.names = c("(Intercept)", "Mother's IQ",
                             "Mother graduated HS", "Mother's age"),
       booktabs = TRUE, dcolumn = TRUE, use.packages = FALSE,
       label = "tab:tab-01",
       caption = "Predicting children's IQ (IQ rescaled by 100, age by 18)",
       fontsize = "footnotesize",
       include.rsquared = FALSE,
       include.adjrs = FALSE,
       include.rmse = FALSE,
       custom.note = ("\\parbox{.6\\linewidth}{\\vspace{2pt}%stars. Standard errors in brackets.\\\\
       DV (outcome) is child's IQ measured at age 3.}"))
@

\end{frame}



\section{Categorical predictors}

\begin{frame}{Categorical predictors}

Linear regression can accommodate them without a problem.\bigskip

Indicator (``dummy'') variables: can take only two values---female (yes/no), country in Europe (yes/no), post-crisis (yes/no) etc.\bigskip

The coefficient for such variables is interpreted as a difference in the levels of $Y$ for the two categories.\bigskip

Multi-category variables can be transformed into a series of indicator variables.

\end{frame}



\begin{frame}{Example: Boston house prices}

<<ch-2, results='asis'>>=
df_boston <- read.spss(file = "../02-data/boston.sav",
                       to.data.frame = TRUE,
                       use.value.labels = FALSE)

df_boston %<>%
    mutate(town = as.character(town),
           town = str_trim(town))


df_neighbor <- df_boston %>%
    group_by(town) %>%
    summarise(medv = mean(medv), 
              crim = mean(crim),
              indus = mean(indus), 
              nox = mean(nox) * 100,
              chas = mean(chas), 
              ptratio = mean(ptratio),
              age = mean(age),
              rooms = mean(rm)) %>%
    mutate(chas_cat = if_else(chas <= 0.5, 0, 1),
           crim_cat = case_when(crim < 5 ~ 0,
                                crim >= 5 & crim < 15 ~ 1,
                                crim >= 15 ~ 2))

model1 <- lm(medv ~ rooms,
             data = df_neighbor,
             na.action = na.omit)
model2 <- lm(medv ~ rooms + chas_cat,
             data = df_neighbor,
             na.action = na.omit)

texreg(list(model1, model2), single.row = FALSE,
       custom.model.names = c("Cont. only", "Cont. and cat."),
       custom.coef.names = c("(Intercept)", "Average num. rooms",
                             "Next to Charles river (dich.)"),
       digits=3, caption = "Comparison of regressions",
       dcolumn = TRUE, booktabs = TRUE, use.packages = FALSE,
       fontsize = "footnotesize",
       include.rsquared = FALSE,
       include.adjrs = FALSE,
       include.rmse = FALSE,
       custom.note = ("\\parbox{.8\\linewidth}{\\vspace{2pt}%stars. Standard errors in brackets.\\\\
       DV (outcome) is average house price in Boston townships (in 1,000s of USD).}"))
@

The model is $Price = a + b_1Rooms + b_2River + e$.

\end{frame}



\begin{frame}{Example: Boston house prices}

<<ch-3, results='asis'>>=
# We can try it again, but this time with the problems solved.
# I will take 6 rooms as the average

df_neighbor %<>%
    mutate(rooms_cent = rooms - 6)

model1 <- lm(medv ~ rooms_cent,
             data = df_neighbor,
             na.action = na.omit)
model2 <- lm(medv ~ rooms_cent + chas_cat,
             data = df_neighbor,
             na.action = na.omit)

texreg(list(model1, model2), single.row = FALSE,
       custom.model.names = c("Cont. only", "Cont. and cat."),
       custom.coef.names = c("(Intercept)", "Average num. rooms",
                             "Next to Charles river (dich.)"),
       digits=3, caption = "Number of rooms rescaled by 6",
       dcolumn = TRUE, booktabs = TRUE, use.packages = FALSE,
       fontsize = "footnotesize",
       label = "table:coefficients-2",
       include.rsquared = FALSE,
       include.adjrs = FALSE,
       include.rmse = FALSE,
       custom.note = ("\\parbox{.8\\linewidth}{\\vspace{2pt}%stars. Standard errors in brackets.\\\\
       DV (outcome) is average house price in Boston townships (in 1,000s of USD).}"))
@

\end{frame}



\begin{frame}[fragile]{Visualizing the model}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.9]
\begin{axis}[
	xlabel=Rooms, % label x axis
	ylabel=House price (1000s USD), % label y axis
	axis lines=left, %set the position of the axes
	xmin=6, xmax=12, % set the min and max values of the x-axis
	ymin=0, ymax=90, % set the min and max values of the y-axis
	clip=false
]

\draw [very thick] (0,200)--(500,750);
\draw [very thick] (0,300)--(500,850);
\draw [thick,->,>=stealth] (100,310)--(100,405) node [midway,right, yshift=2pt] {\scriptsize{$b_2$}};
\draw [thick,->,>=stealth] (200,420)--(300,420) node [midway,below] {\scriptsize{$1$}};
\draw [thick,->,>=stealth] (300,420)--(300,530) node [midway,right] {\scriptsize{$b_1$}};
\node [fill=none] at (550, 700) {\scriptsize{$Charles=0$}};
\node [fill=none] at (550, 880) {\scriptsize{$Charles=1$}};
\draw[decorate,decoration={brace}, thick] (-10,0) -- node[left] {\scriptsize{$a$}} (-10,200);
\draw[decorate,decoration={brace, mirror}, thick] (10,0) -- node[right] {\scriptsize{$a+b_2$}} (10,295);
\end{axis}
\end{tikzpicture}
\caption{Dummy variable regression}
\end{figure}

\end{frame}



\begin{frame}{More than 2 categories}

Turn into a set of indicator variables---28 EU countries turned into 28 variables.\bigskip

For $n$ categories, only $n-1$ indicator variables can be in the regression model. One category must be used as a reference category.\bigskip

Works the same for an indicator variable: 2 categories (male/female) result in only 1 variable included in model.\bigskip

For the Boston price data, take the example of crime rate: low (1--4.99 crimes per 1,000 residents), middle (5--14.99 crimes/1,000), and high (15 crimes and above/1,000).

\end{frame}



\begin{frame}{Example: Boston house prices}

<<ch-4, results='asis'>>=
model3 <- lm(medv ~ rooms_cent + chas_cat + as.factor(crim_cat),
             data = df_neighbor,
             na.action = na.omit)

texreg(list(model2, model3), single.row = FALSE,
       custom.model.names = c("Model 1", "Model 2"),
       custom.coef.names = c("(Intercept)",
                             "Average num. rooms",
                             "Next to Charles river (dich.)",
                             "Moderate crime (5 to 15/1000)",
                             "High crime (over 15/1000)"),
       digits = 3,
       caption = "Excluded category: low crime (1--4.99 per 1,000 residents)",
       dcolumn = TRUE, booktabs = TRUE, use.packages = FALSE,
       fontsize = "scriptsize",
       label = "table:coefficients-3",
       include.rsquared = FALSE,
       include.adjrs = FALSE,
       include.rmse = FALSE,
       custom.note = ("\\parbox{.8\\linewidth}{\\vspace{2pt}%stars. Standard errors in brackets.\\\\
       DV (outcome) is average house price in Boston townships (in 1,000s of USD).}"))
@

\end{frame}



\begin{frame}{Visualizing the model}

\begin{figure}
\centering
\begin{tikzpicture}[x=1.2cm,y=1.2cm,z=0.72cm,>=stealth, scale=0.7]
% The axes
\draw[->] (xyz cs:x=-0.5) -- (xyz cs:x=4.5) node[above] {$Rooms$};
\draw[->] (xyz cs:y=-0.5) -- (xyz cs:y=6.5) node[right] {$Price$};
\draw[->] (xyz cs:z=-0.5) -- (xyz cs:z=2.5) node[right] {$River$};
% The thick ticks for Y
\draw[thick] (-2pt,2) -- (2pt,2) node[left=2pt] {};
\draw[thick] (-2pt,4) -- (2pt,4) node[left=2pt] {};
\draw[thick] (-2pt,6) -- (2pt,6) node[left=2pt] {};
% Ticks for X
\draw[thick] (2,-2pt) -- (2,2pt) node[below=4pt] {};
\draw[thick] (4,-2pt) -- (4,2pt) node[below=4pt] {};
% For Z now I need a special tick
\draw[thick] (xyz cs:y=-0.1pt,z=2) -- (xyz cs:y=0.1pt,z=2) node[left=1pt] {1};

% The origin
\node[align=center] at (1,-1) (ori) {(6,0,0)\\\text{origin}};
\draw[->,help lines,shorten >=3pt] (ori) .. controls (0.75,-0.5) and (0.5,-0.25) .. (0,0,0);

% Draw the plane
\path[fill=gray, dotted, thick, draw=title, fill opacity=0.4] (0,3,0)--(2.5,4,0)--(2.5,5,2)--(0,4,2)--(0,3,0);
\draw[decorate,decoration={brace}] (-0.1,0,0) -- node[left] {\scriptsize{$a$}} (-0.1,3,0);

% Draw the last two planes for the medium and high groups
\path[fill=gray, dotted, thick, draw=title, fill opacity=0.4] (0,3.5,0)--(2.5,4.5,0)--(2.5,5.5,2)--(0,4.5,2)--(0,3.5,0);
\path[fill=gray, dotted, thick, draw=title, fill opacity=0.4] (0,1.5,0)--(2.5,2.5,0)--(2.5,3.5,2)--(0,2.5,2)--(0,1.5,0);
\draw[->, >=stealth] (1,1.89,0) -- (2,1.89,0);
\draw[->, >=stealth] (2,1.89,0) -- (2,2.29,0) node [midway, right] {\tiny{$b_1$}};
\draw[->, >=stealth] (2.5,2.5,0) -- (2.5,2.5,2);
\draw[->, >=stealth] (2.5,2.5,2) -- (2.5,3.5,2) node [midway, right] {\tiny{$b_2$}};
% Final set of braces
\draw[decorate,decoration={brace}] (-0.3,0,0) -- node[left] {\scriptsize{$a+b_3$}} (-0.3,3.5,0);
\draw[decorate,decoration={brace,mirror}] (0.1,0,0) -- node[right] {\scriptsize{$a+b_4$}} (0.1,1.5,0);
\end{tikzpicture}
\end{figure}

\end{frame}





\section{Inference in simple regression}

\begin{frame}{Inference: from sample to population}
How do we know that what we find in a sample is also valid in the population?\bigskip

In the NLSY example, 434 tested children are a sample of the population (all US children aged 3).\bigskip

How much would $a$ and $b$ vary if different samples of size $n$ would be selected?

Ways of presenting this:
\begin{itemize}
\item Standard errors (based on variance)
\item Confidence intervals
\end{itemize}
\end{frame}



\begin{frame}{Sampling variance}

We can compute \textit{how much} the coefficients are expected to change, on average.\bigskip

\begin{align}
V(a) =& \frac{\sigma_e^2\sum_{i=1}^nx_i^2}{n\sum_{i=1}^n(x_i - \bar{x})^2} \\
V(b) =& \frac{\sigma_e^2}{\sum_{i=1}^n(x_i - \bar{x})^2} = \frac{\sigma_e^2}{(n-1)\sigma_x^2}
\end{align}

Small sampling variances are desirable---$a$ and $b$ would not be very different if we randomly selected another sample.

\end{frame}



\begin{frame}{Sampling variance: behavior}

\begin{equation}
V(b) = \frac{\sigma_e^2}{(n-1)\sigma_x^2}
\end{equation}

\begin{itemize}
\item larger $n$ means smaller $V(a)$ and $V(b)$;
\item as $\sigma_e^2$ increases, so do $V(a)$ and $V(b)$;
\item as $\sum_{i=1}^{n}(x_i - \bar{x})^2$ increases, $V(a)$ and $V(b)$ get smaller.
\end{itemize}

The formulas, in fact, use $\sigma_\epsilon^2$, but we have to rely on $\sigma_e^2$ as an approximation ($e_i$=sample residuals; $\epsilon_i$=(hypothetical) population residuals)
\end{frame}


\begin{frame}{Null Hypothesis Significance Testing (NHST)}

A standard $t$ test of the hypothesis that $b \neq 0$, with the null hypothesis $b_0=0$.\bigskip

\begin{equation}
t_0 = \frac{b - b_0}{\sigma_b}
\end{equation}

Because $\sigma_b$ is the standard deviation of an estimated quantity ($b$), it's technically a \textit{standard error}.\bigskip

\end{frame}


\begin{frame}[label=nhstbegin]{Null Hypothesis Significance Testing (NHST)}

\begin{equation}
t_0 = \frac{b - b_0}{\sigma_b}\nonumber
\end{equation}

$t_0$: $t$ distribution with $n-k-1 = n-2$ degrees of freedom.\bigskip

If $t_0$ is larger than the critical value at that level of significance, then the $H_0$ is rejected and $H_1$ is (indirectly) supported.

\hyperlink{morenhst<1>}{\beamerskipbutton{More NHST}}

\end{frame}



\begin{frame}{Confidence intervals}

An alternative way of presenting uncertainty. Once we have $V(a)$ and $V(b)$, they are easy.\bigskip

A $100(1-\alpha)$\% confidence interval for $b$ is:

\begin{equation}
\lbrack b - t_{\frac{\alpha}{2}}\sigma_b; b + t_{\frac{\alpha}{2}}\sigma_b \rbrack
\end{equation}

Critical $t$ values for $\alpha = 0.05$ (two-tailed):

\begin{itemize}
\item $t_{0.025} \approx 1.96$ for $n \geq 500$;
\item $t_{0.025} \approx 2$ for $n \approx 60$;
\item $t_{0.025} \approx 2.1$ for $n \approx 20$;
\end{itemize}

\end{frame}



\begin{frame}{Confidence intervals}

With $b$, $t$ and $V(b)$ we have all the ``ingredients'' to construct the confidence interval (CI).\bigskip

Two aspects are of relevance:

\begin{itemize}
\item the width of the interval---the wider, the more uncertainty we have about $\beta$ (population value);
\item whether it intersects 0---if it does, we can't be sure that $\beta$ is not, in fact, 0.
\end{itemize}

\end{frame}










\section{Inference in multiple regression}


\begin{frame}{The case of multiple regression}

Constructing CIs and conducting NHST is done in the same way as for simple regression.\bigskip

\begin{equation}
V(b_j) = \underbrace{\frac{1}{1 - R_j^2}}_\text{VIF} \times \frac{\sigma_e^2}{\sum_{i=1}^n(x_{j} - \bar{x_j})^2}
\end{equation}

The second part is the same as for simple regression. The first part is called the \textit{variance inflation factor} (VIF).\bigskip

$R_j^2$ is the model fit from a regression of $X_j$ on all the other $X$s (predictors) in the model.

\end{frame}



\begin{frame}{VIF and multicollinearity}

VIF is the reason why we didn't add all 3 indicator variables for crime rate.\bigskip

Knowing the values on 2 of the indicators gives us the value for the third indicator as well.\bigskip

Adding all 3 to the regression means that $R_j^2$ is 1, and that the variance for the indicator variables is $\infty$.\bigskip

This logic also works when correlations between predictors are too high (e.g., above 0.80--0.85).

\end{frame}



\begin{frame}{Inference for multiple slopes: $F$ test}

It can show you whether a model with $k$ predictors fits the data better than a model with no predictors.\bigskip

The null hypothesis for such a test would be:

\begin{equation}
H_0: b_1 = b_2 = \dots = b_k = 0
\end{equation}

This is called a ``global'' test, or an ``omnibus'' test, based on the $F$ distribution.

\end{frame}



\begin{frame}{$F$ test}

\begin{equation}
F_0 = \frac{\frac{RegSS}{k}}{\frac{RSS}{n-k-1}} = \frac{n-k-1}{k} \times \frac{R^2}{1-R^2}
\end{equation}

RegSS and RSS are the same quantities we discussed in the model fit section, $n$ is the sample size, and $k$ is the number of predictors.\bigskip

$F$-statistic has an $F$-distribution with $n$ and $n-k-1$ degrees of freedom.\bigskip

If $F_0$ surpasses the critical value for the test, then $H_0$ is rejected, and you may conclude that at least one of the $b_1$, $b_2$, $\dots$, $b_k$ slopes is different from 0.

\end{frame}




\begin{frame}{$F$ test (cont.)}

It can be used in the same way to see if a model with $k$ predictors fits the data better than a model with fewer predictors.\bigskip

Two models are compared: the ``null'' (predictors we're not interested in), and the ``full'' (all predictors).\bigskip

{\footnotesize
\begin{align}
Null: Y =& a + b_4X_4 + b_5X_5 + e_1 \\
Full: Y =& a + b_1X_1 + b_2X_2 + b_3X_3 + b_4X_4 + b_5X_5 + e_2
\end{align}
}

This this case $H_0: b_1 = b_2 = b_3 = 0$.

\end{frame}



\begin{frame}{$F$ test (cont.)}

\begin{equation}
F_0 = \frac{\frac{RegSS_1 - RegSS_0}{q}}{\frac{RSS_1}{n-k-1}} = \frac{n-k-1}{q} \times \frac{R_1^2 - R_0^2}{1 - R_1^2}
\end{equation}

$RegSS_0$, $RSS_0$, and $R_0^2$ refer to the null model. $RegSS_1$, $RSS_1$, and $R_1^2$ refer to the null model.\bigskip

$n$ is the sample size, $k$ is the number of predictors in the full model (5), and $q$ is the number of predictors in the full model that are not in the null model (3).\bigskip

If the test is significant, then at least one of $b_1$, $b_2$ or $b_3$ is $\neq 0$.

\end{frame}



\begin{frame}{Regression on population data}

What do SEs mean in our California 1992 example, where our regression contains \textit{all} the counties in California?\bigskip

Two strategies:

\begin{enumerate}
\item Only focus on $a$ and $b$ and ignore the $SEs$, as they are meaningless.
\item Adopt the ``superpopulation'' assumption, e.g. the counties in California in 1992 are a sample out of all the possible ways in which counties in California might have developed historically.
\end{enumerate}

The second strategy is accepted in the discipline, particularly if the goal is to make predictions outside of the sample.

\end{frame}


\section{More interpretation}

\begin{frame}{Predicting emancipative values}

<<ch-5, eval=FALSE>>=
# This will require downloading the WVS6 data, and replacing the relative path
# used here with the one valid on your machine.
df_wvs <- read.spss(file = "~/iCloudDrive/Documents/11-Data-repository/19-WVS/WV6_Data_Spss_v20180912.sav",
                    to.data.frame = TRUE,
                    use.value.labels = FALSE)

df_zaf <- df_wvs %>%
    filter(V2 == 710) %>%
    rename(male = V240,
           age = V242) %>%
    mutate(female = if_else(male == 1, 0, 1),
           age = age - 37,
           primar = if_else(V248 %in% c(1, 2, 3), 1, 0),
           secondar = if_else(V248 %in% c(4, 5, 6, 7), 1, 0),
           tertiar = if_else(V248 %in% c(8, 9), 1, 0),
           single = if_else(V57 == 6, 1, 0),
           income = V239,
           buyelect = if_else(V228G %in% c(1, 2), 1, 0),
           supervisor = if_else(V234 == 1, 1, 0)) %>%
    dplyr::select(age, female, secondar, tertiar, single, income,
                  supervisor, RESEMAVAL)
rm(df_wvs)

saveRDS(df_zaf, file = "../02-data/05-zaf-wvs6.rds")
@

In South Africa (from WVS 6):

\begin{itemize}
\item Age: in years (centered at 37 years);
\item Gender: Female = 1; Male = 0;
\item Education: 3 dummies (primary, secondary, tertiary);
\item Marital status: single vs. everyone else;
\item Income: 10 deciles;
\item Supervisor: R. is a supervisor at work.
\end{itemize}\bigskip

Emancipative values are measured on a constructed scale, ranging from 0 to 1.
\end{frame}


\begin{frame}{Results}

\begin{columns}[T] % align columns
  \begin{column}{.70\textwidth}
    
<<ch-6, results='asis'>>=
df_zaf <- readRDS("../02-data/05-zaf-wvs6.rds")

model1 <- lm(RESEMAVAL ~ age + female + secondar + tertiar + single + income,
             na.action = na.omit,
             data = df_zaf)
model2 <- lm(RESEMAVAL ~ age + female + secondar + tertiar + 
               single + income + supervisor,
             na.action = na.omit,
             data = df_zaf)

texreg(list(model1, model2), single.row = TRUE,
       custom.model.names = c("Model 1", "Model 2"),
       custom.coef.names = c("(Intercept)",
                             "Age (centered)",
                             "Gender (female)",
                             "Education (secondary)",
                             "Education (tertiary)",
                             "Marital status (single)",
                             "Income (decile)",
                             "Supervisor at work (yes)"),
       digits=3, caption = "",
       dcolumn = TRUE, booktabs = TRUE, use.packages = FALSE,
       fontsize = "scriptsize",
       label = "table:coefficients-4",
       custom.note = ("\\parbox{\\linewidth}{\\vspace{2pt}%stars. Standard errors in brackets.\\\\
       DV (outcome) is index of emancipative values at individual level (0-1 range).}"))
@
\end{column}%
\hfill%
\begin{column}{.20\textwidth}

\footnotesize{For education, reference category is primary education completed.}
\end{column}%
\end{columns}



\end{frame}



% FRAME
\begin{frame}
\begin{center}
    \Huge Thank \textcolor{title}{you} for the kind attention!
\end{center}
\end{frame}

\appendix

\begin{frame}[label=morenhst]{NHST}

Step 1: Under the null hypothesis ($H_0$) we expect $\beta=0$ in the population.\bigskip

Step 2: Our sample regression, however, produced a $\beta = b$, with a standard error of $SE = s$.\bigskip

Step 3: A t-test for whether $b$ is statistically significant is $t=\frac{b - 0}{s}$.
\end{frame}


\begin{frame}{NHST: final steps}

Step 4: Compare this $t$ value with a critical value of the t-test, for the \# of d.f. in your regression, at a 5\% significance level (probability of rejecting $H_0$ when it is true).\bigskip

Step 5: If our $t$ result is larger, $\beta$ is statistically significantly different from 0 in the population. If $t$ is smaller, it is not.

\end{frame}


\begin{frame}{NHST}

Thankfully, the software does this automatically. Given the size of the coefficient and SE, it can compute very precisely the probability that we would see an effect as strong as $\beta$ \textit{if $H_0$ were true}.\bigskip

All we need to do as researchers is check if $p<.05$.\bigskip

If $p>.05$, then we cannot be sure that the effect in the population is not, in fact, 0.

\end{frame}


\begin{frame}{NHST (reality is messy and painful)}

\begin{figure}
\centering
\includegraphics[scale=0.5]{../04-graphs/P-values}
\end{figure}

\hyperlink{nhstbegin<1>}{\beamerreturnbutton{Go Back}}

\end{frame}


\end{document}
